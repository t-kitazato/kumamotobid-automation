from selenium import webdriver
from selenium.webdriver.support.ui import Select
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import UnexpectedAlertPresentException
from selenium.webdriver.common.alert import Alert
from bs4 import BeautifulSoup
from datetime import datetime
import os
import time
import traceback
import csv
import pandas as pd
import sys

# âœ… å¼•æ•°å—ã‘å–ã‚Š
if len(sys.argv) < 3:
    print("Usage: python announcement_info.py <Excelãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹> <ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€> [é–‹å§‹æ—¥] [çµ‚äº†æ—¥]")
    sys.exit(1)

excel_path   = sys.argv[1]
download_dir = sys.argv[2]
start_date   = sys.argv[3] if len(sys.argv) > 3 else None
end_date     = sys.argv[4] if len(sys.argv) > 4 else None

print(f"ğŸ“Š Excelãƒ•ã‚¡ã‚¤ãƒ«: {excel_path}")
print(f"ğŸ“ ä¿å­˜å…ˆ: {download_dir}")
print(f"ğŸ“… æ¸¡ã•ã‚ŒãŸæœŸé–“ï¼ˆæœªä½¿ç”¨ï¼‰: {start_date} ï½ {end_date}")

# âœ… ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åˆæœŸåŒ–
log_path = os.path.join(download_dir, "found_seko_nos.csv")
if not os.path.exists(log_path):
    with open(log_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["æ–½å·¥ç•ªå·", "ç¨®åˆ¥", "ãƒ•ã‚¡ã‚¤ãƒ«å", "å–å¾—æ™‚åˆ»"])

write_header = not os.path.exists("download_log.csv")
valid_exts = ['.pdf', '.xls', '.xlsx', '.doc', '.docx', '.zip']

# âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ã‚’å¾…ã¤é–¢æ•°
def wait_for_download_complete(download_dir, filename, timeout=30):
    path = os.path.join(download_dir, filename)
    for _ in range(timeout):
        if os.path.exists(path) and not filename.endswith(".crdownload"):
            return True
        time.sleep(1)
    return False

# âœ… ãƒ¡ã‚¤ãƒ³å‡¦ç†
def download_bid_files(seko_nos, download_dir):
    options = Options()
    options.add_experimental_option("prefs", {
        "download.default_directory": download_dir,
        "download.prompt_for_download": False,
        "download.directory_upgrade": True,
        "safebrowsing.enabled": True,
        "profile.default_content_setting_values.automatic_downloads": 1,
        "profile.default_content_setting_values.popups": 0
    })
    options.add_argument("--disable-popup-blocking")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument('--headless')

    driver = webdriver.Chrome(options=options)

    try:
        # ğŸŒ ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã¸
        driver.get("https://ebid.kumamoto-idc.pref.kumamoto.jp/PPIAccepter/TopServlet")
        WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, '//img[contains(@src, "logo_kumamoto_pref.gif")]'))
        ).click()
        time.sleep(3)

        # ğŸ“‹ å·¦ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰æ¤œç´¢ç”»é¢ã¸
        driver.switch_to.default_content()
        WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmLEFT")))
        driver.execute_script("jsLink(3,1);")
        time.sleep(3)

        # ğŸ” æ¤œç´¢æ¡ä»¶æŒ‡å®š
        driver.switch_to.default_content()
        WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
        WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmTOP")))

        Select(driver.find_element(By.NAME, "GYOSYU_TYPE")).select_by_visible_text("å·¥äº‹")
        Select(driver.find_element(By.NAME, "HACHU_TANTOU_KYOKU")).select_by_visible_text("é˜¿è˜‡åœ°åŸŸæŒ¯èˆˆå±€")

        driver.find_element(By.NAME, "btnSearch").click()
        time.sleep(2)

        found_seko_nos = set()

        while True:
            driver.switch_to.default_content()
            WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
            WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmMIDDLE")))

            html = driver.page_source
            soup = BeautifulSoup(html, "html.parser")
            table = soup.find("table", {"id": "data"})
            rows = table.find("tbody").find_all("tr") if table else []

            for i, row in enumerate(rows):
                cells = row.find_all("td")
                if len(cells) >= 7:
                    seko_no = next(cells[0].stripped_strings)
                    if seko_no in seko_nos and seko_no not in found_seko_nos:
                        found_seko_nos.add(seko_no)
                        print(f"âœ… è©²å½“æ–½å·¥ç•ªå·ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {seko_no} â†’ jsInfo({i}) å®Ÿè¡Œ")

                        project_dir = os.path.join(download_dir, seko_no)
                        os.makedirs(project_dir, exist_ok=True)

                        driver.switch_to.default_content()
                        WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
                        WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmMIDDLE")))
                        driver.execute_script(f"jsInfo({i});")
                        time.sleep(2)

                        try:
                            WebDriverWait(driver, 3).until(EC.alert_is_present())
                            alert = Alert(driver)
                            print(f"âš ï¸ ã‚¢ãƒ©ãƒ¼ãƒˆæ¤œå‡º: {alert.text}")
                            alert.accept()
                        except:
                            pass

                        driver.switch_to.default_content()
                        WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
                        WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmBOTTOM")))

                        detail_html = driver.page_source
                        with open(os.path.join(project_dir, f"detail_{seko_no}.html"), "w", encoding="utf-8") as f:
                            f.write(detail_html)

                        file_ids = ['01', '02', '03', '04', '05', '06', '10']
                        for fid in file_ids:
                            print(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {fid}")
                            soup = BeautifulSoup(driver.page_source, "html.parser")
                            onclick_pattern = f"jsDoubleClick_Check({i}, '{fid}')"
                            found_link = soup.find("img", onclick=lambda x: x and onclick_pattern in x)

                            if not found_link:
                                print(f"ğŸš« ãƒ•ã‚¡ã‚¤ãƒ« {fid} ã®ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ â†’ ã‚¹ã‚­ãƒƒãƒ—")
                                continue

                            before_files = set(os.listdir(download_dir))
                            driver.execute_script(f"jsDoubleClick_Check({i}, '{fid}');")
                            time.sleep(3)

                            downloaded_files = set(os.listdir(download_dir)) - before_files
                            for fname in downloaded_files:
                                if not fname.lower().endswith(tuple(valid_exts)):
                                    continue

                                if not wait_for_download_complete(download_dir, fname):
                                    print(f"âš ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æœªå®Œäº†: {fname}")
                                    continue

                                base, ext = os.path.splitext(fname)
                                new_name = f"{seko_no}_{fid}_{base}{ext}"
                                dst_path = os.path.join(project_dir, new_name)

                                if os.path.exists(dst_path):
                                    base, ext = os.path.splitext(new_name)
                                    new_name = f"{base}_{int(time.time())}{ext}"
                                    dst_path = os.path.join(project_dir, new_name)

                                try:
                                    os.rename(os.path.join(download_dir, fname), dst_path)
                                    print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•æˆåŠŸ: {fname} â†’ {new_name}")
                                    with open(log_path, "a", newline="", encoding="utf-8") as f:
                                        writer = csv.writer(f)
                                        writer.writerow([seko_no, fid, new_name, time.strftime("%Y-%m-%d %H:%M:%S")])
                                except Exception as e:
                                    print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•å¤±æ•—: {fname} â†’ {new_name} ({type(e).__name__})")

                            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆæ–½å·¥ç•ªå·é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
                            for fname in os.listdir(download_dir):
                                if fname.lower().endswith(tuple(valid_exts)) and seko_no in fname:
                                    try:
                                        os.remove(os.path.join(download_dir, fname))
                                    except Exception as e:
                                        print(f"âš ï¸ å‰Šé™¤å¤±æ•—: {fname} ({type(e).__name__})")

            # â­ï¸ æ¬¡ãƒšãƒ¼ã‚¸ã¸
            driver.switch_to.default_content()
            WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
            WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmMIDDLE")))

            with open(os.path.join(download_dir, "frmTOP_snapshot.html"), "w", encoding="utf-8") as f:
                f.write(driver.page_source)

            try:
                next_btn = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.NAME, "btnNextPage")))
                disabled = next_btn.get_attribute("disabled")
                if disabled is not None:
                    print("ğŸ“„ æœ€çµ‚ãƒšãƒ¼ã‚¸ã«åˆ°é”ã—ã¾ã—ãŸ")
                    break
                print("â­ï¸ æ¬¡ãƒšãƒ¼ã‚¸ã¸ç§»å‹•ã—ã¾ã™")
                driver.execute_script("jsNextPage();")
                WebDriverWait(driver, 10).until(
                    lambda d: "æ¬¡é " not in d.page_source or "å‰é " in d.page_source
                )
                time.sleep(2)
            except Exception as e:
                print(f"âš ï¸ æ¬¡ãƒšãƒ¼ã‚¸ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {type(e).__name__}")
                break

        # âš ï¸ æœªå–å¾—æ–½å·¥ç•ªå·ã®è¡¨ç¤ºã¨ä¿å­˜
        not_found = set(seko_nos) - found_seko_nos
        if not_found:
            print("âš ï¸ ä»¥ä¸‹ã®æ–½å·¥ç•ªå·ã¯æ¤œç´¢çµæœã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ:")
            with open(os.path.join(download_dir, "not_found_seko_nos.csv"), "w", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow(["æœªå–å¾—æ–½å·¥ç•ªå·", "ç¢ºèªæ™‚åˆ»"])
                for nf in not_found:
                    print(f" - {nf}")
                    writer.writerow([nf, time.strftime("%Y-%m-%d %H:%M:%S")])

    finally:
        driver.quit()
        print("ğŸ›‘ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’çµ‚äº†ã—ã¾ã—ãŸ")

# âœ… å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯
if __name__ == "__main__":
    df = pd.read_excel(excel_path)
    seko_nos = df["æ–½å·¥ç•ªå·"].dropna().astype(str).tolist()

    print(f"ğŸ“Š æ–½å·¥ç•ªå·ä¸€è¦§: {seko_nos}")
    download_bid_files(seko_nos, download_dir)
    print("âœ… å…¥æœ­æƒ…å ±ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")