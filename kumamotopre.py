import logging
import re
import time
import pandas as pd
import sys
import os
import merge_bid_data
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
from selenium.webdriver.chrome.options import Options

# âœ… å¼•æ•°å—ã‘å–ã‚Š
if len(sys.argv) < 3:
    print("Usage: python kumamotopre.py <ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€> <Excelãƒ•ã‚¡ã‚¤ãƒ«> [é–‹å§‹æ—¥] [çµ‚äº†æ—¥]")
    sys.exit(1)

excel_file  = sys.argv[1]
save_folder = sys.argv[2]
start_date = sys.argv[3] if len(sys.argv) > 3 else None
end_date = sys.argv[4] if len(sys.argv) > 4 else None

print(f"ğŸ“ ä¿å­˜å…ˆ: {save_folder}")
print(f"ğŸ“‹ Excelãƒ•ã‚¡ã‚¤ãƒ«: {excel_file}")
print(f"ğŸ“… æ¸¡ã•ã‚ŒãŸæœŸé–“ï¼ˆæœªä½¿ç”¨ï¼‰: {start_date} ï½ {end_date}")


# âœ… ãƒ­ã‚°è¨­å®š
log_path = os.path.join(save_folder, "kumamoto_scraper.log")
logging.basicConfig(filename=log_path, level=logging.INFO,
                    format="%(asctime)s - %(levelname)s - %(message)s")
logging.info("ğŸ“Œ ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹")

# âœ… ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ
os.makedirs(save_folder, exist_ok=True)

# âœ… ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
def extract_text(soup, label):
    cell = soup.find("td", string=label)
    return cell.find_next("td").text.strip() if cell else "å–å¾—å¤±æ•—"

def clean_price(text):
    match = re.search(r"\d[\d,]*å††", text)
    return match.group(0) if match else text

def extract_location(soup):
    for label in ["å·¥äº‹å ´æ‰€", "å ´æ‰€", "æ–½å·¥å ´æ‰€"]:
        value = extract_text(soup, label)
        if value != "å–å¾—å¤±æ•—":
            return value
    return "å…¥æœ­å‰"

def extract_award_info(soup):
    for row in soup.find_all("tr"):
        cells = row.find_all("td")
        if len(cells) >= 3 and "è½æœ­" in cells[-1].text:
            return clean_price(cells[1].text.strip()), cells[0].text.strip()
    return "å…¥æœ­å‰", "å…¥æœ­å‰"

def contains_target_vendor(soup, target="åŒ—é‡Œé“è·¯ï¼ˆæ ªï¼‰"):
    return target in soup.get_text()

# âœ… ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•

options = Options()
headless_flag = os.environ.get("HEADLESS", "True")
if headless_flag == "True":
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
driver = webdriver.Chrome(options=options)
driver.get("https://ebid.kumamoto-idc.pref.kumamoto.jp/PPIAccepter/TopServlet")
WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//img[contains(@src, "logo_kumamoto_pref.gif")]'))).click()
time.sleep(1)

# å·¦ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰æ¤œç´¢ç”»é¢ã¸
driver.switch_to.default_content()
WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmLEFT")))
driver.execute_script("jsLink(1,1);")
time.sleep(1)

# æ¤œç´¢æ¡ä»¶æŒ‡å®š
driver.switch_to.default_content()
WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmTOP")))

Select(driver.find_element(By.NAME, "NYUSATU_TYPE")).select_by_visible_text("é€šå¸¸å‹æŒ‡åç«¶äº‰å…¥æœ­")
Select(driver.find_element(By.NAME, "GYOSYU_TYPE")).select_by_visible_text("å·¥äº‹")
Select(driver.find_element(By.NAME, "HACHU_TANTOU_KYOKU")).select_by_visible_text("é˜¿è˜‡åœ°åŸŸæŒ¯èˆˆå±€")

driver.find_element(By.NAME, "btnSearch").click()
time.sleep(1)

# ğŸ“¦ çµæœæ ¼ç´
bid_data = []
page_count = 0

# ğŸ” ãƒšãƒ¼ã‚¸å·¡å›
while True:
    page_count += 1
    driver.switch_to.default_content()
    WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
    WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmMIDDLE")))

    rows = driver.find_elements(By.XPATH, '//table[@id="data"]/tbody/tr')
    logging.info(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page_count}: {len(rows)} ä»¶ã®æ¡ˆä»¶ã‚’æ¤œå‡º")

    for index in range(len(rows)):
        try:
            rows = driver.find_elements(By.XPATH, '//table[@id="data"]/tbody/tr')
            img = rows[index].find_element(By.XPATH, './/img[contains(@onclick, "jsBidInfo")]')
            onclick_code = img.get_attribute("onclick").replace("javascript:", "").replace(";", "")
            driver.execute_script(onclick_code)
            logging.info(f"ğŸ–±ï¸ jsBidInfo å®Ÿè¡Œ: {onclick_code}")
            time.sleep(2)

            driver.switch_to.default_content()
            WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
            WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmBOTTOM")))

            soup = BeautifulSoup(driver.page_source, "html.parser")
            if contains_target_vendor(soup):
                logging.info(f"âœ… æŒ‡åæ¥­è€…ã«ã€åŒ—é‡Œé“è·¯ï¼ˆæ ªï¼‰ã€ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
                bid_data.append([
                    extract_text(soup, "æ–½è¡Œç•ªå·"),
                    extract_text(soup, "å·¥äº‹ãƒ»æ¥­å‹™å"),
                    "é€šå¸¸å‹æŒ‡åç«¶äº‰å…¥æœ­",
                    extract_text(soup, "é–‹æœ­ï¼ˆäºˆå®šï¼‰æ—¥"),
                    "",
                    clean_price(extract_text(soup, "äºˆå®šä¾¡æ ¼")),
                    extract_location(soup),
                    *extract_award_info(soup)
                ])
            else:
                logging.info(f"â›” æŒ‡åæ¥­è€…ã«ã€åŒ—é‡Œé“è·¯ï¼ˆæ ªï¼‰ã€ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—")

            driver.execute_script("jsBack();")
            time.sleep(1)
            driver.switch_to.default_content()
            WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
            WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmMIDDLE")))

        except Exception as e:
            logging.error(f"âŒ è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

    # âœ… ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†
    try:
        driver.switch_to.default_content()
        WebDriverWait(driver, 5).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
        WebDriverWait(driver, 5).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmMIDDLE")))

        soup = BeautifulSoup(driver.page_source, "html.parser")
        next_button = soup.find("input", {"name": "btnNextPage", "type": "button", "value": "æ¬¡é "})
        if next_button and "disabled" not in next_button.attrs:
            logging.info("â¡ï¸ æ¬¡ãƒšãƒ¼ã‚¸ã¸ç§»å‹•ï¼ˆjsNextPage å®Ÿè¡Œï¼‰")
            driver.execute_script("jsNextPage();")
            time.sleep(2)
        else:
            logging.info("ğŸ“˜ æœ€çµ‚ãƒšãƒ¼ã‚¸ã«åˆ°é”ã€‚å‡¦ç†çµ‚äº†")
            break
    except Exception as e:
        logging.error(f"âŒ ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
        break

# ğŸ“¤ Excelå‡ºåŠ›
output_path = os.path.join(save_folder, "åŒ—é‡Œé“è·¯_å…¥æœ­å€™è£œæ¡ˆä»¶.xlsx")
df = pd.DataFrame(bid_data, columns=[
    "æ–½å·¥ç•ªå·", "å·¥äº‹ãƒ»æ¥­å‹™å", "å…¥æœ­åŠã³å¥‘ç´„æ–¹æ³•", "é–‹æœ­äºˆå®šæ—¥",
        "å‚™è€ƒ", "äºˆå®šä¾¡æ ¼", "å·¥äº‹å ´æ‰€", "è½æœ­é‡‘é¡", "è½æœ­æ¥­è€…"
])
df.to_excel(output_path, index=False)
logging.info(f"ğŸ“¦ Excelä¿å­˜å®Œäº†: {output_path}")

# ğŸ›‘ çµ‚äº†å‡¦ç†
driver.quit()
logging.info("ğŸ›‘ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’çµ‚äº†ã—ã¾ã—ãŸ")
print("âœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

#ã€€å…¥æœ­ç®¡ç†.xlsx ã¸åæ˜ 
existing_excel = "å…¥æœ­ç®¡ç†data.xlsx"
new_excel = output_path  # å–å¾—çµæœã®Excelãƒ‘ã‚¹
merge_bid_data.merge_preserving_format(existing_excel, new_excel)

