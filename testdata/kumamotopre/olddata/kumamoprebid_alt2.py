import logging
import re
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup

# âœ… ãƒ­ã‚°è¨­å®š
logging.basicConfig(filename="kumamoto_scraper.log", level=logging.INFO,
                    format="%(asctime)s - %(levelname)s - %(message)s")
logging.info("ğŸ“Œ ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹")

# ğŸŒ ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
driver = webdriver.Chrome()
driver.get("https://ebid.kumamoto-idc.pref.kumamoto.jp/PPIAccepter/TopServlet")
WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//img[contains(@src, "logo_kumamoto_pref.gif")]'))).click()
time.sleep(1)

# å·¦ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰æ¤œç´¢ç”»é¢ã¸
driver.switch_to.default_content()
WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmLEFT")))
driver.execute_script("jsLink(1,1);")
time.sleep(1)

# æ¤œç´¢æ¡ä»¶æŒ‡å®š
driver.switch_to.default_content()
WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmTOP")))

Select(driver.find_element(By.NAME, "NYUSATU_TYPE")).select_by_visible_text("é€šå¸¸å‹æŒ‡åç«¶äº‰å…¥æœ­")
Select(driver.find_element(By.NAME, "GYOSYU_TYPE")).select_by_visible_text("å·¥äº‹")
Select(driver.find_element(By.NAME, "HACHU_TANTOU_KYOKU")).select_by_visible_text("é˜¿è˜‡åœ°åŸŸæŒ¯èˆˆå±€")

driver.find_element(By.NAME, "btnSearch").click()
time.sleep(1)

# âœ… ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
def extract_text(soup, label):
    cell = soup.find("td", string=label)
    return cell.find_next("td").text.strip() if cell else "å–å¾—å¤±æ•—"

def clean_price(text):
    match = re.search(r"\d[\d,]*å††", text)
    return match.group(0) if match else text

def extract_location(soup):
    for label in ["å·¥äº‹å ´æ‰€", "å ´æ‰€", "æ–½å·¥å ´æ‰€"]:
        value = extract_text(soup, label)
        if value != "å–å¾—å¤±æ•—":
            return value
    return "å–å¾—å¤±æ•—"

def extract_award_info(soup):
    for row in soup.find_all("tr"):
        cells = row.find_all("td")
        if len(cells) >= 3 and "è½æœ­" in cells[-1].text:
            return clean_price(cells[1].text.strip()), cells[0].text.strip()
    return "å–å¾—å¤±æ•—", "å–å¾—å¤±æ•—"

def contains_target_vendor(soup, target="åŒ—é‡Œé“è·¯ï¼ˆæ ªï¼‰"):
    return target in soup.get_text()

# ğŸ“¦ çµæœæ ¼ç´
bid_data = []
page_count = 0

# ğŸ” ãƒšãƒ¼ã‚¸å·¡å›
while True:
    page_count += 1
    driver.switch_to.default_content()
    WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
    WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmMIDDLE")))

    rows = driver.find_elements(By.XPATH, '//table[@id="data"]/tbody/tr')
    logging.info(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page_count}: {len(rows)} ä»¶ã®æ¡ˆä»¶ã‚’æ¤œå‡º")

    for index in range(len(rows)):
        try:
            # âœ… row ã‚’æ¯å›å†å–å¾—ï¼ˆStaleElement å›é¿ï¼‰
            rows = driver.find_elements(By.XPATH, '//table[@id="data"]/tbody/tr')
            img = rows[index].find_element(By.XPATH, './/img[contains(@onclick, "jsBidInfo")]')
            onclick_code = img.get_attribute("onclick").replace("javascript:", "").replace(";", "")
            driver.execute_script(onclick_code)
            logging.info(f"ğŸ–±ï¸ jsBidInfo å®Ÿè¡Œ: {onclick_code}")
            time.sleep(2)

             # è©³ç´°ãƒšãƒ¼ã‚¸ã¸ç§»å‹•
            driver.switch_to.default_content()
            WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
            WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmBOTTOM")))

            soup = BeautifulSoup(driver.page_source, "html.parser")
            if contains_target_vendor(soup):
                logging.info(f"âœ… æŒ‡åæ¥­è€…ã«ã€åŒ—é‡Œé“è·¯ï¼ˆæ ªï¼‰ã€ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
                bid_data.append([
                    extract_text(soup, "æ–½è¡Œç•ªå·"),
                    extract_text(soup, "å·¥äº‹ãƒ»æ¥­å‹™å"),
                    "é€šå¸¸å‹æŒ‡åç«¶äº‰å…¥æœ­",
                    extract_text(soup, "é–‹æœ­ï¼ˆäºˆå®šï¼‰æ—¥"),
                    "",
                    clean_price(extract_text(soup, "äºˆå®šä¾¡æ ¼")),
                    extract_location(soup),
                    *extract_award_info(soup)
                ])
            else:
                logging.info(f"â›” æŒ‡åæ¥­è€…ã«ã€åŒ—é‡Œé“è·¯ï¼ˆæ ªï¼‰ã€ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—")

            # âœ… æˆ»ã‚‹ â†’ frmMIDDLE ã«æˆ»ã‚‹
            driver.execute_script("jsBack();")
            time.sleep(1)
            driver.switch_to.default_content()
            WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
            WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmMIDDLE")))

        except Exception as e:
            logging.error(f"âŒ è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")


    # âœ… ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†
    try:
        driver.switch_to.default_content()
        WebDriverWait(driver, 5).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmRIGHT")))
        WebDriverWait(driver, 5).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, "frmMIDDLE")))

        soup = BeautifulSoup(driver.page_source, "html.parser")
        next_button = soup.find("input", {"name": "btnNextPage", "type": "button", "value": "æ¬¡é "})
        if next_button and "disabled" not in next_button.attrs:
            logging.info("â¡ï¸ æ¬¡ãƒšãƒ¼ã‚¸ã¸ç§»å‹•ï¼ˆjsNextPage å®Ÿè¡Œï¼‰")
            driver.execute_script("jsNextPage();")
            time.sleep(2)
        else:
            logging.info("ğŸ“˜ æœ€çµ‚ãƒšãƒ¼ã‚¸ã«åˆ°é”ã€‚å‡¦ç†çµ‚äº†")
            break
    except Exception as e:
        logging.error(f"âŒ ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
        break

# ğŸ“¤ Excelå‡ºåŠ›
df = pd.DataFrame(bid_data, columns=[
    "æ–½å·¥ç•ªå·", "å·¥äº‹ãƒ»æ¥­å‹™å", "å…¥æœ­åŠã³å¥‘ç´„æ–¹æ³•", "é–‹æœ­äºˆå®šæ—¥",
    "å‚™è€ƒ", "äºˆå®šä¾¡æ ¼", "å·¥äº‹å ´æ‰€", "è½æœ­é‡‘é¡", "è½æœ­æ¥­è€…"
])
df.to_excel("åŒ—é‡Œé“è·¯_å…¥æœ­å€™è£œæ¡ˆä»¶.xlsx", index=False)
logging.info("ğŸ“¦ Excelä¿å­˜å®Œäº†: åŒ—é‡Œé“è·¯_å…¥æœ­å€™è£œæ¡ˆä»¶.xlsx")

# ğŸ›‘ çµ‚äº†
driver.quit()
logging.info("ğŸ›‘ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’çµ‚äº†ã—ã¾ã—ãŸ")
